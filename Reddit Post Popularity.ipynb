{"cells":[{"cell_type":"markdown","source":["## Overview\n\nThis notebook will show you how to create and query a table or DataFrame that you uploaded to DBFS. [DBFS](https://docs.databricks.com/user-guide/dbfs-databricks-file-system.html) is a Databricks File System that allows you to store data for querying inside of Databricks. This notebook assumes that you have a file already inside of DBFS that you would like to read from.\n\nThis notebook is written in **Python** so the default cell type is Python. However, you can use different languages by using the `%LANGUAGE` syntax. Python, Scala, SQL, and R are all supported."],"metadata":{}},{"cell_type":"code","source":["from pyspark.sql.functions import isnan, when, count, col, monotonically_increasing_id, udf\nimport pyspark.sql.functions as f\nfrom pyspark.ml.feature import StringIndexer, VectorAssembler, OneHotEncoder\nfrom pyspark.ml.stat import Correlation\nimport sparknlp\nfrom sparknlp.pretrained import PretrainedPipeline\n!pip install googletrans\nfrom googletrans import Translator\nfrom pyspark.sql.types import StringType\nfrom pyspark.ml import Pipeline\nfrom pyspark.sql.window import Window\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns"],"metadata":{},"outputs":[],"execution_count":2},{"cell_type":"code","source":["# File location and type\nfile_location = \"/FileStore/tables/RS_v2_2006_03.json\"\nfile_type = \"json\"\n\n# CSV options\ninfer_schema = \"true\"\nfirst_row_is_header = \"true\"\ndelimiter = \",\"\n\n\n# The applied options are for CSV files. For other file types, these will be ignored.\ndf = spark.read.format(file_type) \\\n  .option(\"inferSchema\", infer_schema) \\\n  .option(\"header\", first_row_is_header) \\\n  .option(\"sep\", delimiter) \\\n  .load(file_location)\n\ndf_id = df.withColumn(\"id_1\", monotonically_increasing_id())\ndf_id.createOrReplaceTempView('df_temp')\ndf_id = spark.sql('select *, row_number() over (order by \"idx\") as index from df_temp').drop('id_1')\n\ndisplay(df_id)"],"metadata":{},"outputs":[],"execution_count":3},{"cell_type":"markdown","source":["Slicing the dataframe"],"metadata":{}},{"cell_type":"code","source":["df1 = df_id.select(df_id.columns[:])\n# display(df1)"],"metadata":{},"outputs":[],"execution_count":5},{"cell_type":"markdown","source":["Dataframe description"],"metadata":{}},{"cell_type":"code","source":["df1.count()"],"metadata":{},"outputs":[],"execution_count":7},{"cell_type":"code","source":["display(df1.describe())"],"metadata":{},"outputs":[],"execution_count":8},{"cell_type":"code","source":["# counting the null values in the dataframe\ndisplay(df1.select([count(when(col(c).isNull(), c)).alias(c) for c in df1.columns]))"],"metadata":{},"outputs":[],"execution_count":9},{"cell_type":"code","source":["display(df1.groupBy(\"subreddit_id\").count())"],"metadata":{},"outputs":[],"execution_count":10},{"cell_type":"code","source":["display(df1.groupBy(\"subreddit_name_prefixed\").count())"],"metadata":{},"outputs":[],"execution_count":11},{"cell_type":"markdown","source":["Seems like the above 2 columns are correlated, thus checking their correlation"],"metadata":{}},{"cell_type":"code","source":["indexer = StringIndexer(inputCol=\"subreddit_id\", outputCol=\"subreddit_idIndex\")\ndf1 = indexer.fit(df1).transform(df1)\ndisplay(df1)"],"metadata":{},"outputs":[],"execution_count":13},{"cell_type":"code","source":["indexer = StringIndexer(inputCol=\"subreddit_name_prefixed\", outputCol=\"subreddit_name_prefixedIndex\")\ndf1 = indexer.fit(df1).transform(df1)\ndisplay(df1)"],"metadata":{},"outputs":[],"execution_count":14},{"cell_type":"code","source":["dfcorr1 = df1.select(\"subreddit_name_prefixedIndex\", \"subreddit_idIndex\")\ndisplay(dfcorr1)"],"metadata":{},"outputs":[],"execution_count":15},{"cell_type":"code","source":["# convert to vector column first\nassembler = VectorAssembler(inputCols=dfcorr1.columns, outputCol=\"corr_features\")\ndf_vector = assembler.transform(dfcorr1).select(\"corr_features\")\n\n# get correlation matrix\nmatrix = Correlation.corr(df_vector, \"corr_features\")\nmatrix.collect()[0][\"pearson({})\".format(\"corr_features\")].values"],"metadata":{},"outputs":[],"execution_count":16},{"cell_type":"code","source":["display(df1.groupBy(\"subreddit_type\").count())"],"metadata":{},"outputs":[],"execution_count":17},{"cell_type":"code","source":["display(df1.groupBy(\"thumbnail\").count())"],"metadata":{},"outputs":[],"execution_count":18},{"cell_type":"markdown","source":["Removing suggested_sort, thumbnail_height, thumbnail_width as the columns have more than 10000 null values. Thumbnail column is removed because the data is highly imbalanced which will not help in model building. Since Subreddit_name_prefixed and subreddit_id is 100% correlated it's best to select only one of these columns."],"metadata":{}},{"cell_type":"code","source":["df1n = df1.select([c for c in df1.columns if c not in {'suggested_sort','thumbnail_height','thumbnail_width','thumbnail','subreddit_name_prefixed', 'subreddit_name_prefixedIndex', 'subreddit_id', 'subreddit_idIndex'}])\ndisplay(df1n)"],"metadata":{},"outputs":[],"execution_count":20},{"cell_type":"markdown","source":["##Case1 - Null replaced as a third variable"],"metadata":{}},{"cell_type":"code","source":["df1n = df1n.fillna({\"whitelist_status\":0})\ndisplay(df1n.groupBy(\"whitelist_status\").count())"],"metadata":{},"outputs":[],"execution_count":22},{"cell_type":"code","source":["indexer = StringIndexer(inputCol=\"whitelist_status\", outputCol=\"whitelist_statusIndex\")\ndf1n = indexer.fit(df1n).transform(df1n)\ndisplay(df1n)"],"metadata":{},"outputs":[],"execution_count":23},{"cell_type":"code","source":["dfcorr = df1n.select(\"score\", \"whitelist_statusIndex\")\ndisplay(dfcorr)"],"metadata":{},"outputs":[],"execution_count":24},{"cell_type":"code","source":["# convert to vector column first\nassembler = VectorAssembler(inputCols=dfcorr.columns, outputCol=\"corr_features\")\ndf_vector = assembler.transform(dfcorr).select(\"corr_features\")\n\n# get correlation matrix\nmatrix = Correlation.corr(df_vector, \"corr_features\")\nmatrix.collect()[0][\"pearson({})\".format(\"corr_features\")].values"],"metadata":{},"outputs":[],"execution_count":25},{"cell_type":"markdown","source":["## Case2 - Null replaced with most frequent variable"],"metadata":{}},{"cell_type":"code","source":["df1 = df1.fillna({\"whitelist_status\":\"all_ads\"})\ndisplay(df1.groupBy(\"whitelist_status\").count())"],"metadata":{},"outputs":[],"execution_count":27},{"cell_type":"code","source":["indexer = StringIndexer(inputCol=\"whitelist_status\", outputCol=\"whitelist_statusIndex\")\ndf1 = indexer.fit(df1).transform(df1)\ndfcorr = df1.select(\"score\", \"whitelist_statusIndex\")\n\n# convert to vector column first\nassembler = VectorAssembler(inputCols=dfcorr.columns, outputCol=\"corr_features\")\ndf_vector = assembler.transform(dfcorr).select(\"corr_features\")\n\n# get correlation matrix\nmatrix = Correlation.corr(df_vector, \"corr_features\")\nmatrix.collect()[0][\"pearson({})\".format(\"corr_features\")].values"],"metadata":{},"outputs":[],"execution_count":28},{"cell_type":"markdown","source":["Since imputing the null with median reduces the correlation as well as makes the data more imbalanced thus, we will not impute the null values with median instead we will take it as a third variable."],"metadata":{}},{"cell_type":"code","source":["indexer = StringIndexer(inputCol=\"subreddit_type\", outputCol=\"subreddit_typeIndex\")\ndf1n = indexer.fit(df1n).transform(df1n)\ndisplay(df1n)"],"metadata":{},"outputs":[],"execution_count":30},{"cell_type":"code","source":["# make a temp dataframe using input column labelled as text\ndf_text_title = df.select(\"title\")\n# df_text_title = df_text_title.withColumnRenamed(\"title\", \"text\")\ndisplay(df_text_title)"],"metadata":{},"outputs":[],"execution_count":31},{"cell_type":"code","source":["# https://stackoverflow.com/questions/54811428/why-am-i-getting-a-modulenotfounderror-when-using-googletrans\n# https://stackoverflow.com/questions/59714502/python-translate-a-column-with-multiple-languages-to-english\ndef trans(x):\n  translator=Translator()\n  return translator.translate(x, dest=\"en\").text\n\nlation = udf(trans)\nspark.udf.register(\"lation\", lation)\ndf_text_title = df_text_title.withColumn(\"translated text\", lation(\"title\"))\ndf_text_title1 = df_text_title.select(\"translated text\")\n# df_text_title2 = df_text_title2.withColumnRenamed(\"translated text\", \"text\")\ndisplay(df_text_title1)"],"metadata":{},"outputs":[],"execution_count":32},{"cell_type":"code","source":["def clean_str(x):\n  punc = ''';:,.|'''\n  for ch in x:\n    if ch in punc:\n      x = x.replace(ch, '')\n  return x\nclean = udf(clean_str)\nspark.udf.register(\"clean\", clean)\ndf_text_title1 = df_text_title1.withColumn(\"text\", clean(\"translated text\"))\ndf_text_title2 = df_text_title1.select(\"text\")\n# display(df_text_title2)"],"metadata":{},"outputs":[],"execution_count":33},{"cell_type":"code","source":["# #initialize NER pipeline. It takes col with name text\npipeline = PretrainedPipeline(\"analyze_sentiment\", lang=\"en\")\ndf_transformed = pipeline.transform(df_text_title2)"],"metadata":{},"outputs":[],"execution_count":34},{"cell_type":"code","source":["#store the results in a temporary dataframe\ndf_new = df_transformed.select(\"sentiment.result\", \"text\")\ndisplay(df_new)"],"metadata":{},"outputs":[],"execution_count":35},{"cell_type":"code","source":["spark.conf.set(\"spark.sql.autoBroadcastJoinThreshold\", -1)\n#join temperorary dataframe to original dataframe\ndf_new = df_new.withColumn(\"id_1\", monotonically_increasing_id())\ndf_new.createOrReplaceTempView('df_temp1')\ndf_new = spark.sql('select *, row_number() over (order by \"idx\") as index1 from df_temp1').drop('id_1')\ndf_full = df1n.join(df_new, df1n.index == df_new.index1).orderBy(df1n.index).drop(\"index1\", \"title\")\n# display(df_full)"],"metadata":{},"outputs":[],"execution_count":36},{"cell_type":"code","source":["#cache statement, do all feature engineering transformations, then perform the display action in the end\ndf_full.cache()"],"metadata":{},"outputs":[],"execution_count":37},{"cell_type":"code","source":[" #https://stackoverflow.com/questions/48927271/count-number-of-words-in-a-spark-dataframe\ndf_full = df_full.withColumn(\"length_title\", f.length(\"text\"))\ndf_full = df_full.withColumn('titlewordCount', f.size(f.split(f.col('text'), ' ')))            \ndf_full = df_full.withColumn('url length', f.length(f.split(f.col('url'), '//').getItem(1)))\n# display(df_full)"],"metadata":{},"outputs":[],"execution_count":38},{"cell_type":"code","source":["#has exclamation mark in the end?\ndf_full = df_full.withColumn( \\\n          'has_exclamation_mark', f.when(f.col(\"text\").endswith('!') == True , 1) \\\n          .otherwise(0))\n\n#has question mark in the end?\ndf_full = df_full.withColumn( \\\n          'has_question_mark', f.when(f.col(\"text\").endswith('?') == True , 1) \\\n          .otherwise(0))"],"metadata":{},"outputs":[],"execution_count":39},{"cell_type":"code","source":["df_full.registerTempTable('dfexplor')\ndfex = sqlContext.sql(\"SELECT has_question_mark, avg(score) as avg_score from dfexplor group by has_question_mark \")\ndfex = dfex.toPandas()\ndfex.plot.bar(x = \"has_question_mark\", rot = 0)"],"metadata":{},"outputs":[],"execution_count":40},{"cell_type":"code","source":["dfex1 = sqlContext.sql(\"SELECT has_exclamation_mark, avg(score) as avg_score from dfexplor group by has_exclamation_mark \")\ndfex1 = dfex1.toPandas()\ndfex1.plot.bar(x = \"has_exclamation_mark\", rot = 0)"],"metadata":{},"outputs":[],"execution_count":41},{"cell_type":"code","source":["# def get_last_part(x):\n#   return str(x).split(\"//\")[-1].split('/')[0].split('.')[-1]\n# # String type output\n\n# get_last_part_udf = udf(get_last_part, StringType())\n# spark.udf.register(\"get_last_part_udf\", get_last_part_udf)\n# df_full = df_full.withColumn(\"top domain url\", get_last_part_udf('url'))\n# display(df_full)"],"metadata":{},"outputs":[],"execution_count":42},{"cell_type":"code","source":["def last(x):\n  return str(x).split('.')[-1]\n# String type output\n\nlast_udf = udf(last, StringType())\nspark.udf.register(\"last_udf\", last_udf)\ndf_full = df_full.withColumn(\"last\", last_udf('domain'))\n# display(df_full)"],"metadata":{},"outputs":[],"execution_count":43},{"cell_type":"code","source":["def protocol(x):\n  return str(x).split(\":\")[0]\n\n# String type output\nprotocol_udf = udf(protocol, StringType())\nspark.udf.register(\"protocol_udf\", protocol_udf)\ndf_full = df_full.withColumn(\"protocol\", protocol_udf('url'))\ndf_full = df_full.drop(\"subreddit_type\", \"url\", \"whitelist_status\", \"text\")\ndisplay(df_full)"],"metadata":{},"outputs":[],"execution_count":44},{"cell_type":"code","source":["# display(df_full.groupBy(\"top domain url\").count())"],"metadata":{},"outputs":[],"execution_count":45},{"cell_type":"code","source":["dfs = df_full.select(\"*\").toPandas()\nplt.figure(figsize = (15, 10))\nsx = sns.distplot(dfs[\"score\"], kde=False)\nsx.set_title(\"Histogram of Popularity\", fontsize = 15)\nsx.set_xlabel(\"Popularity\", fontsize = 15)\nsx.set_ylabel(\"Frequency\", fontsize = 15)"],"metadata":{},"outputs":[],"execution_count":46},{"cell_type":"code","source":["plt.figure(figsize = (15, 10))\npx = sns.distplot(dfs[\"length_title\"], kde=False)\npx.set_title(\"Histogram of number of characters in Title\", fontsize = 15)\npx.set_xlabel(\"Title length\", fontsize = 15)\npx.set_ylabel(\"Frequency\", fontsize = 15)"],"metadata":{},"outputs":[],"execution_count":47},{"cell_type":"code","source":["display(df_full.groupBy(\"last\").count().orderBy('count', ascending=False))"],"metadata":{},"outputs":[],"execution_count":48},{"cell_type":"code","source":["display(df_full.groupBy(\"last\").count())"],"metadata":{},"outputs":[],"execution_count":49},{"cell_type":"code","source":["# display(df_full.groupBy(\"protocol\").count())"],"metadata":{},"outputs":[],"execution_count":50},{"cell_type":"code","source":["# display(df_full.select([count(when(col(c).isNull(), c)).alias(c) for c in df_full.columns]))"],"metadata":{},"outputs":[],"execution_count":51},{"cell_type":"code","source":["# display(df_full.groupBy(\"lastIndex\").count().orderBy('lastIndex'))"],"metadata":{},"outputs":[],"execution_count":52},{"cell_type":"code","source":["# File location and type\nfile_location = \"/FileStore/tables/top500Domains.csv\"\nfile_type = \"csv\"\n\n# CSV options\ninfer_schema = \"true\"\nfirst_row_is_header = \"true\"\ndelimiter = \",\"\n\n# The applied options are for CSV files. For other file types, these will be ignored.\ndf500 = spark.read.format(file_type) \\\n  .option(\"inferSchema\", infer_schema) \\\n  .option(\"header\", first_row_is_header) \\\n  .option(\"sep\", delimiter) \\\n  .load(file_location)\n\ndisplay(df500)"],"metadata":{},"outputs":[],"execution_count":53},{"cell_type":"code","source":["df500 = df500.withColumn(\"last_domain\", last_udf('Root Domain'))\nwindowSpec = Window.partitionBy(df500['last_domain'])\ndf500 = df500.withColumn(\"average da\", f.avg(df500['Domain Authority']).over(windowSpec)).orderBy('Rank')\ndisplay(df500)"],"metadata":{},"outputs":[],"execution_count":54},{"cell_type":"code","source":["dflast = df500.select('last_domain', 'average da').distinct()\ndisplay(dflast)"],"metadata":{},"outputs":[],"execution_count":55},{"cell_type":"code","source":["df_full = df_full.join(dflast, df_full.last == dflast.last_domain)\ndf_full = df_full.fillna({\"average_da\": 1})\n# display(df_full)"],"metadata":{},"outputs":[],"execution_count":56},{"cell_type":"code","source":["df_full = df_full.withColumn(\"result1\", df_full[\"result\"].getItem(1)) \\\n          .withColumn(\"result\", df_full[\"result\"].getItem(0)).drop('last_domain')\n\ndf_full = df_full.fillna({\"result\":\"none\"})\nindexer = StringIndexer(inputCol=\"result\", outputCol=\"resultIndex\")\ndf_full = indexer.fit(df_full).transform(df_full)\n\nindexer1 = StringIndexer(inputCol=\"last\", outputCol=\"lastIndex\")\ndf_full = indexer1.fit(df_full).transform(df_full)\n\nindexer2 = StringIndexer(inputCol=\"protocol\", outputCol=\"protocolIndex\")\ndf_full = indexer2.fit(df_full).transform(df_full)\n\ndf_full = df_full.fillna({\"result1\":\"none\"})\nindexer3 = StringIndexer(inputCol=\"result1\", outputCol=\"result1Index\")\ndf_full = indexer3.fit(df_full).transform(df_full)\n\ndf_full = df_full.drop(\"result\", \"last\", \"protocol\", \"result1\")\n\n# display(df_full)"],"metadata":{},"outputs":[],"execution_count":57},{"cell_type":"code","source":["train_pdf = df_full.select(\"length_title\", \"titlewordCount\", \"url length\", \"has_exclamation_mark\", \"has_question_mark\", \"average da\", \"whitelist_statusIndex\",\\\n                           \"subreddit_typeIndex\", \"resultIndex\", \"lastIndex\", \"protocolIndex\", \"result1Index\").toPandas()\n\ncorr = train_pdf.corr()\n\nplt.figure(figsize = (10, 10))\nax = sns.heatmap(corr, vmin=-1, vmax=1, center=0, cmap=sns.diverging_palette(20, 220, n=200), square=True, annot = True, fmt = \".2f\")\nax.set_xticklabels(ax.get_xticklabels(), rotation=45, horizontalalignment='right')"],"metadata":{},"outputs":[],"execution_count":58},{"cell_type":"code","source":["df_train = df_full.select(df_full.columns[:])\n\nohe = OneHotEncoder(dropLast=False, inputCol=\"whitelist_statusIndex\", outputCol=\"whitelist_status_ohe\")\n# model = ohe.fit(df_full)\ndf_train = ohe.transform(df_train)\n\nohe1 = OneHotEncoder(dropLast=False, inputCol=\"subreddit_typeIndex\", outputCol=\"subreddit_type_ohe\")\n# model1 = ohe1.fit(df_full)\ndf_train = ohe1.transform(df_train)\n\nohe2 = OneHotEncoder(dropLast=False, inputCol=\"resultIndex\", outputCol=\"result_ohe\")\n# model2 = ohe2.fit(df_full)\ndf_train = ohe2.transform(df_train)\n\ndf_train = df_train.fillna({\"lastIndex\":100})\nohe3 = OneHotEncoder(dropLast=False, inputCol=\"lastIndex\", outputCol=\"last_ohe\")\n# model3 = ohe3.fit(df_full)\ndf_train = ohe3.transform(df_train)\n\nohe4 = OneHotEncoder(dropLast=False, inputCol=\"protocolIndex\", outputCol=\"protocol_ohe\")\n# model4 = ohe4.fit(df_full)\ndf_train = ohe4.transform(df_train)\n\nohe5 = OneHotEncoder(dropLast=False, inputCol=\"result1Index\", outputCol=\"result1_ohe\")\n# model5 = ohe5.fit(df_full)\ndf_train = ohe5.transform(df_train)\n\ndf_train = df_train.drop(\"whitelist_statusIndex\", \"subreddit_typeIndex\", \"resultIndex\", \"protocolIndex\", \"result1Index\", \"lastIndex\")\n\ndisplay(df_train)"],"metadata":{},"outputs":[],"execution_count":59},{"cell_type":"markdown","source":["##Test Data"],"metadata":{}},{"cell_type":"code","source":["# File location and type\nfile_location = \"/FileStore/tables/RS_v2_2006_04.json\"\nfile_type = \"json\"\n\n# CSV options\ninfer_schema = \"true\"\nfirst_row_is_header = \"true\"\ndelimiter = \",\"\n\n\n# The applied options are for CSV files. For other file types, these will be ignored.\ndf = spark.read.format(file_type) \\\n  .option(\"inferSchema\", infer_schema) \\\n  .option(\"header\", first_row_is_header) \\\n  .option(\"sep\", delimiter) \\\n  .load(file_location)\n\ndf_id = df.withColumn(\"id_1\", monotonically_increasing_id())\ndf_id.createOrReplaceTempView('df_temp')\ndf_id = spark.sql('select *, row_number() over (order by \"idx\") as index from df_temp').drop('id_1')\n\ndisplay(df_id)"],"metadata":{},"outputs":[],"execution_count":61},{"cell_type":"code","source":["df1 = df_id.select([c for c in df_id.columns if c not in {'suggested_sort','thumbnail_height','thumbnail_width','thumbnail','subreddit_name_prefixed', 'subreddit_id'}])\ndisplay(df1)"],"metadata":{},"outputs":[],"execution_count":62},{"cell_type":"code","source":["df1 = df1.fillna({\"whitelist_status\":0})\nindexer = StringIndexer(inputCol=\"whitelist_status\", outputCol=\"whitelist_statusIndex\")\ndf1 = indexer.fit(df1).transform(df1)\ndf1 = df1.drop(\"whitelist_status\")\ndisplay(df1)"],"metadata":{},"outputs":[],"execution_count":63},{"cell_type":"code","source":["indexer = StringIndexer(inputCol=\"subreddit_type\", outputCol=\"subreddit_typeIndex\")\ndf1 = indexer.fit(df1).transform(df1)\ndf1 = df1.drop(\"subreddit_type\")\ndisplay(df1)"],"metadata":{},"outputs":[],"execution_count":64},{"cell_type":"code","source":["# make a temp dataframe using input column labelled as text\ndf_text_title = df1.select(\"title\")\ndf_text_title = df_text_title.withColumn(\"translated text\", lation(\"title\"))\ndf_text_title1 = df_text_title.select(\"translated text\")\ndf_text_title1 = df_text_title1.withColumn(\"text\", clean(\"translated text\"))\ndf_text_title2 = df_text_title1.select(\"text\")\n# display(df_text_title2)"],"metadata":{},"outputs":[],"execution_count":65},{"cell_type":"code","source":["# #initialize NER pipeline. It takes col with name text\npipeline = PretrainedPipeline(\"analyze_sentiment\", lang=\"en\")\n\n# Transform 'data' and store output in a new 'annotations_df' dataframe\ndf_transformed = pipeline.transform(df_text_title2)\n\ndf_new = df_transformed.select(\"sentiment.result\", \"text\")\ndisplay(df_new)"],"metadata":{},"outputs":[],"execution_count":66},{"cell_type":"code","source":["spark.conf.set(\"spark.sql.autoBroadcastJoinThreshold\", -1)\n#join temperorary dataframe to original dataframe\ndf_new = df_new.withColumn(\"id_1\", monotonically_increasing_id())\ndf_new.createOrReplaceTempView('df_temp1')\ndf_new = spark.sql('select *, row_number() over (order by \"idx\") as index1 from df_temp1').drop('id_1')\ndf_fu = df1.join(df_new, df1.index == df_new.index1).orderBy(df1.index).drop(\"index1\", \"title\")\n# display(df_fu)"],"metadata":{},"outputs":[],"execution_count":67},{"cell_type":"code","source":["# cache \ndf_fu.cache()"],"metadata":{},"outputs":[],"execution_count":68},{"cell_type":"code","source":["df_fu = df_fu.withColumn(\"length_title\", f.length(\"text\"))\ndf_fu = df_fu.withColumn('titlewordCount', f.size(f.split(f.col('text'), ' '))) #https://stackoverflow.com/questions/48927271/count-number-of-words-in-a-spark-dataframe\ndf_fu = df_fu.withColumn('url length', f.length(f.split(f.col('url'), '//').getItem(1)))\n\ndf_fu = df_fu.withColumn('has_exclamation_mark', f.when(f.col(\"text\").endswith('!') == True , 1).otherwise(0))\ndf_fu = df_fu.withColumn('has_question_mark', f.when(f.col(\"text\").endswith('?') == True , 1).otherwise(0))\n\ndf_fu = df_fu.withColumn(\"last\", last_udf('domain'))\ndf_fu = df_fu.withColumn(\"protocol\", protocol_udf('url')).drop(\"url\", \"text\")\n\ndf_fu = df_fu.join(dflast, df_fu.last == dflast.last_domain).drop('last_domain')\ndf_fu = df_fu.fillna({\"average da\": 1})\n\n# display(df_fu)"],"metadata":{},"outputs":[],"execution_count":69},{"cell_type":"code","source":["df_fu = df_fu.withColumn(\"result1\", df_fu[\"result\"].getItem(1)).withColumn(\"result\", df_fu[\"result\"].getItem(0))\n\ndf_fu = df_fu.fillna({\"result\":\"none\"})\nindexer = StringIndexer(inputCol=\"result\", outputCol=\"resultIndex\")\ndf_fu = indexer.fit(df_fu).transform(df_fu)\n\ndf_fu = df_fu.fillna({\"result1\":\"none\"})\n\npipeline = Pipeline(stages=[indexer1, indexer2, indexer3])\ndf_fu = pipeline.fit(df_fu).transform(df_fu)\n\ndf_fu = df_fu.drop(\"result\", \"last\", \"protocol\", \"result1\")\n\n# display(df_fu)"],"metadata":{},"outputs":[],"execution_count":70},{"cell_type":"code","source":["df_test = df_fu.select(df_fu.columns[:])\n\ndf_test = df_test.fillna({\"lastIndex\":100})\n\npipeline1 = Pipeline(stages=[ohe, ohe1, ohe2, ohe3 ,ohe4, ohe5])\ndf_test = pipeline1.fit(df_test).transform(df_test)\n\ndf_test = df_test.drop(\"whitelist_statusIndex\", \"subreddit_typeIndex\", \"resultIndex\", \"protocolIndex\", \"result1Index\", \"lastIndex\")\n\ndisplay(df_test)"],"metadata":{},"outputs":[],"execution_count":71}],"metadata":{"name":"1628 Proj","notebookId":2440592797382054},"nbformat":4,"nbformat_minor":0}
